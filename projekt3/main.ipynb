{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🤨 How do people feel about Hasanabi's political views?\n",
    "\n",
    "A little of backstory first. I'm a 2nd year uni student on a computer science course. One of my classes is computational intelligence, where we are introduced to the world of machine learning algorithms and ai. Our last assignment of the semester is to use a Twitter scraper and analyze feelings of people engaging in some topic. I wanted to make this interesting so I chose something that would make me finish this. When I asked my friend on suggestions what should I do he suggested me Hasan.\n",
    "\n",
    "He is a popular twitch streamer, internet personality, ¿socialist activist? (for sure someone with socialistic views). He has an online following and is overall liked by the internet crowd, I too engaged with his works (mainly entertainment). Because of his political and economic views he could be named someone that raises a lot of noise and makes people feel a certain way so he is a perfect match for a subject of this project!\n",
    "\n",
    "What we will do is we will download a lot of Mr. Piker's tweets and people's comments under those posts. We will put them through language analyzing algorithms and see how people feel about his political views.\n",
    "\n",
    "DISCLAIMER: Before we continue I'd like to add that this is mainly for entertainment and you SHOULD NOT base any thoughts on it. You should remember that this was made by a uni CS student. ML (machine learning) is also something flawed:\n",
    "\n",
    "- it generates slightly different outcomes every time it's run\n",
    "- is based on math\n",
    "- is something problematic because it's hard to explain a computer how does \"feelings\" and human language work\n",
    "\n",
    "But we can laugh about what will turn out of this. Maybe I could even get Hasan to show it on stream? So without further ado let's continue!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/hubertkowalski/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/hubertkowalski/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/hubertkowalski/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importing python libraries nothing special\n",
    "import pandas as pd\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "from snscrape.modules.twitter import Tweet\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from typing import Union\n",
    "import nltk\n",
    "from dateutil.parser import parse as date_parse\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import matplotlib.pyplot as plt\n",
    "import text2emotion as te\n",
    "from wordcloud import WordCloud\n",
    "import json\n",
    "import html\n",
    "import re\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the data\n",
    "\n",
    "We will now proceed to the code that will make a search request on twitter site and scrape everything we want from it's result. For that we will use snscraper's twitter module.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving Hasan's twitter handle for later use\n",
    "HASAN_TWITTER_HANDLE = \"hasanthehun\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function that gets parameters from us parses it into a valid twitter api query\n",
    "# gets tweets and returns them as a list\n",
    "def get_hasan_tweets(start_date: datetime, end_date: datetime, num_tweets: int = -1) -> list[Tweet]:\n",
    "    tweets: list[Tweet] = []\n",
    "\n",
    "    query = f\"from:{HASAN_TWITTER_HANDLE} since:{start_date.strftime('%Y-%m-%d')} until:{end_date.strftime('%Y-%m-%d')}\"\n",
    "    for tweet in sntwitter.TwitterSearchScraper(query).get_items():\n",
    "        tweets.append(tweet)\n",
    "\n",
    "        if len(tweets) == num_tweets:\n",
    "            break\n",
    "\n",
    "    return tweets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a test request to twitter!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = get_hasan_tweets(\n",
    "    date_parse('2023-03-05'), date_parse('2023-03-06'), num_tweets=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@MoistCr1TiKaL 4 companies practically own their own territories and for some reason we can’t hold them accountable despite the billions we have spent building and maintaining the infastructure they now own 🤔'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[0].rawContent\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay it works! Now for the less fun part.\n",
    "\n",
    "Because, at the time of me writing this, Twitter has turned off, literally two days ago, a search filter on their site that is STILL in their API query reference in their docs. I cannot use conversation_id filter to search only for replies to a tweet. We need to get creative. (THANK YOU ELON!)\n",
    "\n",
    "We will get every reply to hasan and then match them to their respectable \"parent\" tweet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_replies_to_hasan(start_date: datetime, end_date: datetime, num_tweets: int = -1) -> list[Tweet]:\n",
    "    replies: list[Tweet] = []\n",
    "\n",
    "    query = f\"to:{HASAN_TWITTER_HANDLE} since:{start_date.strftime('%Y-%m-%d')} until:{end_date.strftime('%Y-%m-%d')} filter:replies\"\n",
    "    for tweet in sntwitter.TwitterSearchScraper(query).get_items():\n",
    "        replies.append(tweet)\n",
    "\n",
    "        if len(replies) == num_tweets:\n",
    "            break\n",
    "\n",
    "    return replies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "replies = get_replies_to_hasan(date_parse(\n",
    "    '2023-03-05'), date_parse('2023-03-06'), num_tweets=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@hasanthehun Why are you talking shit already the game isn’t even out\n",
      "@hasanthehun Good, it’s about time\n",
      "@hasanthehun I'm sad there won't be any more IRL Japan streams - loved them so much @hasanthehun\n",
      "@hasanthehun I stg the accent at the end racist as hell\n",
      "@hasanthehun Marginalized populations? The alphabet bunch is calling all the shots in the democrat party right now.\n"
     ]
    }
   ],
   "source": [
    "for reply in replies[:5]:\n",
    "    print(reply.rawContent)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our filters work. So now we're going to get every Hasan's tweet and every reply. It'll be a lot of data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function that will get only the data we want from a tweet\n",
    "# We don't want to save data that we won't be using\n",
    "def parse_post(tweet: Tweet):\n",
    "    parsed_tweet = {}\n",
    "    parsed_tweet['id'] = tweet.id\n",
    "    parsed_tweet['url'] = tweet.url\n",
    "    parsed_tweet['date'] = tweet.date.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    parsed_tweet['rawContent'] = tweet.rawContent\n",
    "    parsed_tweet['renderedContent'] = tweet.renderedContent\n",
    "    parsed_tweet['conversationId'] = tweet.conversationId\n",
    "    parsed_tweet['lang'] = tweet.lang\n",
    "    parsed_tweet['inReplyToTweetId'] = tweet.inReplyToTweetId\n",
    "\n",
    "    return parsed_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unavailable user in card on tweet 1480964717443047426\n",
      "User 27073265 not found in user refs in card on tweet 1480964717443047426\n",
      "Stopping after 20 empty pages\n"
     ]
    }
   ],
   "source": [
    "tweets = get_hasan_tweets(date_parse('2020-01-01'), datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parsing tweets\n",
    "parsed_tweets = []\n",
    "for tweet in tweets:\n",
    "    parsed_tweets.append(parse_post(tweet))\n",
    "\n",
    "\n",
    "# Saving them to a json file\n",
    "with open(\"hasan_tweets.json\", \"w\") as file:\n",
    "    json.dump([parsed_tweet for parsed_tweet in parsed_tweets], file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unavailable user in card on tweet 1660148601408966658\n",
      "User 25662595 not found in user refs in card on tweet 1660148601408966658\n",
      "Could not translate t.co card URL on tweet 1637852460390686721\n",
      "Stopping after 20 empty pages\n"
     ]
    }
   ],
   "source": [
    "replies = get_replies_to_hasan(date_parse('2020-01-01'), datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing replies\n",
    "parsed_replies = []\n",
    "for reply in replies:\n",
    "    parsed_replies.append(parse_post(reply))\n",
    "\n",
    "# Saving them to a json file\n",
    "with open(\"hasan_replies.json\", \"w\") as file:\n",
    "    json.dump([parsed_reply for parsed_reply in parsed_replies], file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of that we got 11.7 thousand tweets from hasan (he do post a lot) and 84.5 thousand replies at him. This is not a humongous amount of data but is big enough for our experiments. I decided to get all of the tweets from 1st of January 2020 until 27th of May 2023 (the day of writing this). This should include many interesting events. COVID-19, BLM protests, US election and recent stuff, probably comments about current post COVID economic state of the US.\n",
    "\n",
    "Because of those interesting events and hope that Hasan engaged in the conversation on those topics should stir up some conversations and most importantly extreme feelings (I'd like to remind you that we are still talking about twitter).\n",
    "\n",
    "Our data also includes things like Hasan announcing that he will be live soon, replies to his work friends and other non-political topics. Those tweets should encourage people to reply in a positive way and this would make our data with a tendency to lean in to more positive feelings.\n",
    "\n",
    "We will see what will turn out of this.\n",
    "\n",
    "Our data look something like this:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"id\": 1662165779498795010, // id of the tweet\n",
    "  \"url\": \"https://twitter.com/hasanthehun/status/1662165779498795010\", // tweets url\n",
    "  \"date\": \"2023-05-26 18:36:18\", // date in a string format\n",
    "  \"rawContent\": \"DEBT CEILING QUESTIONS ANSWERED W/ @ddayen + HOGWATCH...\", // raw content of the tweet it's normalized to ASCII\n",
    "  \"renderedContent\": \"DEBT CEILING QUESTIONS ANSWERED W/ @ddayen + HOGWATCH...\", // content how it's being showed in a browser\n",
    "  \"conversationId\": 1662165779498795010, // id of the conversation, if it's a normal tweet it is its id\n",
    "  \"lang\": \"en\", // lang that has been detected by twitter\n",
    "  \"inReplyToTweetId\": null // if it's a reply here it shows to which tweet\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11714, 8)\n"
     ]
    }
   ],
   "source": [
    "# Loading normalized tweets and replies\n",
    "hasan_tweets = pd.read_json('hasan_tweets.json')\n",
    "\n",
    "print(hasan_tweets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84488, 8)\n"
     ]
    }
   ],
   "source": [
    "replies_to_hasan = pd.read_json('hasan_replies.json')\n",
    "\n",
    "print(replies_to_hasan.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Before we continue we need to make a little bit of preprocessing. We need to remove any duplicates from the replies, replies with links (they might be bots promoting scam), and empty tweets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70540, 8)\n"
     ]
    }
   ],
   "source": [
    "# Removing duplicates\n",
    "replies_to_hasan.drop_duplicates(subset=['id', \"rawContent\"], inplace=True)\n",
    "\n",
    "# Removing tweets that contain @@@@@@@@ or http or https\n",
    "replies_to_hasan = replies_to_hasan[~replies_to_hasan['rawContent'].str.contains(\n",
    "    '@@@@@@@@')]\n",
    "replies_to_hasan = replies_to_hasan[~replies_to_hasan['rawContent'].str.contains(\n",
    "    'http://')]\n",
    "replies_to_hasan = replies_to_hasan[~replies_to_hasan['rawContent'].str.contains(\n",
    "    'https://')]\n",
    "\n",
    "# Printing shape of the data frame (how much data we are left with)\n",
    "print(replies_to_hasan.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have to do something with html codes. What are they? You see everything on the web is written in html. This is a markup language that eases formatting text. Html has some that do some things in it but we also want to put them into the text. Those characters are \"<\", \">\" and others like \"/\". So they are not encoded into html code, which is what we have in our data. What we have to do is map those symbols into ASCII.\n",
    "\n",
    "Also we gotta do something with emojis. They also are represented in a string of characters like \"\\ud83d\\ude14\" which is translated to \"😔\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a helper function that will remove all emojis from a string\n",
    "def remove_emojis(text: str) -> str:\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to remove any twitter handles, since they also doesn't contribute much to the overall meaning of the sentence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_twitter_handles(text: str) -> str:\n",
    "    return re.sub(r'@\\w+', '', text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will apply this function and unescape any html entities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "hasan_tweets['rawContent'] = hasan_tweets['rawContent'].apply(\n",
    "    html.unescape).apply(remove_emojis).apply(remove_twitter_handles)\n",
    "\n",
    "hasan_tweets['renderedContent'] = hasan_tweets['renderedContent'].apply(\n",
    "    html.unescape).apply(remove_emojis).apply(remove_twitter_handles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    id                                                url   \n",
      "0  1662165779498795010  https://twitter.com/hasanthehun/status/1662165...  \\\n",
      "1  1661955536580132864  https://twitter.com/hasanthehun/status/1661955...   \n",
      "2  1661874461170348035  https://twitter.com/hasanthehun/status/1661874...   \n",
      "3  1661800157409873920  https://twitter.com/hasanthehun/status/1661800...   \n",
      "4  1661784494301646848  https://twitter.com/hasanthehun/status/1661784...   \n",
      "\n",
      "                 date                                         rawContent   \n",
      "0 2023-05-26 18:36:18  DEBT CEILING QUESTIONS ANSWERED W/  + HOGWATCH...  \\\n",
      "1 2023-05-26 04:40:52   if you tell them you watch me they will elimi...   \n",
      "2 2023-05-25 23:18:42                             this is a gay man btw    \n",
      "3 2023-05-25 18:23:26  RON DESASTEROUS CAMPAIGN LAUNCH, OATHKEEPER LE...   \n",
      "4 2023-05-25 17:21:12                   we are never getting healthcare.   \n",
      "\n",
      "                                     renderedContent       conversationId   \n",
      "0  DEBT CEILING QUESTIONS ANSWERED W/  + HOGWATCH...  1662165779498795010  \\\n",
      "1   if you tell them you watch me they will elimi...  1661918058888433664   \n",
      "2                             this is a gay man btw   1661874461170348035   \n",
      "3  RON DESASTEROUS CAMPAIGN LAUNCH, OATHKEEPER LE...  1661800157409873920   \n",
      "4                   we are never getting healthcare.  1661784494301646848   \n",
      "\n",
      "  lang  inReplyToTweetId  \n",
      "0   en               NaN  \n",
      "1   en      1.661918e+18  \n",
      "2   en               NaN  \n",
      "3   en               NaN  \n",
      "4   en               NaN  \n"
     ]
    }
   ],
   "source": [
    "print(hasan_tweets.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "replies_to_hasan['rawContent'] = replies_to_hasan['rawContent'].apply(\n",
    "    html.unescape).apply(remove_emojis).apply(remove_twitter_handles)\n",
    "\n",
    "replies_to_hasan['renderedContent'] = replies_to_hasan['renderedContent'].apply(\n",
    "    html.unescape).apply(remove_emojis).apply(remove_twitter_handles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    id                                                url   \n",
      "0  1662244477254205441  https://twitter.com/AutarkistThe/status/166224...  \\\n",
      "1  1662242888279900160  https://twitter.com/johnsonjohnsceo/status/166...   \n",
      "2  1662240517697028098  https://twitter.com/ClaudeScuba/status/1662240...   \n",
      "3  1662239613983883265  https://twitter.com/finlay_glasgow/status/1662...   \n",
      "4  1662237863231074304  https://twitter.com/Pray4TheBatman/status/1662...   \n",
      "\n",
      "                 date                                         rawContent   \n",
      "0 2023-05-26 23:49:01   Gays had it coming since 2015. It will get wo...  \\\n",
      "1 2023-05-26 23:42:42    Grifter hogs like multimillionaire Hasan beg...   \n",
      "2 2023-05-26 23:33:17                    Hasan endorses child castration   \n",
      "3 2023-05-26 23:29:41   “The literal using of child mannequins” there...   \n",
      "4 2023-05-26 23:22:44   And? He's not part of the LGBT flag cult. Tar...   \n",
      "\n",
      "                                     renderedContent       conversationId   \n",
      "0   Gays had it coming since 2015. It will get wo...  1661380288805588993  \\\n",
      "1    Grifter hogs like multimillionaire Hasan beg...  1661502529207775236   \n",
      "2                    Hasan endorses child castration  1661380288805588993   \n",
      "3   “The literal using of child mannequins” there...  1661874461170348035   \n",
      "4   And? He's not part of the LGBT flag cult. Tar...  1661874461170348035   \n",
      "\n",
      "  lang  inReplyToTweetId  \n",
      "0   en      1.661380e+18  \n",
      "1   en      1.661503e+18  \n",
      "2   en      1.661380e+18  \n",
      "3   en      1.661874e+18  \n",
      "4   en      1.661874e+18  \n"
     ]
    }
   ],
   "source": [
    "print(replies_to_hasan.head())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create a stopwords list. Stopwords are words like \"a\", \"and\", \"the\", \"an\", so words that do not change the meaning of the sentence. We will also add Hasan's name second name, twitter handle and his twitch username. What we are doing here is called lemmatization. The more you know.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we']\n"
     ]
    }
   ],
   "source": [
    "stopwords_list = stopwords.words('english')\n",
    "stopwords.encoding('')\n",
    "stopwords_list.extend(\n",
    "    ['hasan', \"parker\", \"hasanthehun\", \"hasanabi\"])\n",
    "\n",
    "print(stopwords_list[:5])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a function that will tokenize the tweets. Tokenizing refers to the process of breaking down a text or a sentence into smaller units called tokens. Tokens are typically words, but they can also be phrases, sentences, or even individual characters, depending on the level of granularity desired. This will make our tweets easier to analyze by our NLP (natural language processing) algorithms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text: str) -> list[str]:\n",
    "    words = nltk.word_tokenize(text)\n",
    "    return [word.lower() for word in words if word.isalpha()]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will apply our stopwords to the tweets. This is called \"lemmatization\". It is the process of reducing words to their base or canonical form. The lemma represents the dictionary or canonical form of a word, from which all inflected forms (such as different tenses, plurals, or derivations) are derived. It is used to reduce different forms of a word to a common base. This is because they will be treated as the same item by our algorithms. If they were different they could make some differences making our results inaccurate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def lemmatize(text: str) -> list[str]:\n",
    "    words = [word for word in text if word not in stopwords_list]\n",
    "    return [lemmatizer.lemmatize(word) for word in words]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will apply this to our data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "hasan_tweets['rawContent'] = hasan_tweets['rawContent'].apply(\n",
    "    tokenize).apply(lemmatize)\n",
    "\n",
    "hasan_tweets['renderedContent'] = hasan_tweets['renderedContent'].apply(\n",
    "    tokenize).apply(lemmatize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "replies_to_hasan['rawContent'] = replies_to_hasan['rawContent'].apply(\n",
    "    tokenize).apply(lemmatize)\n",
    "\n",
    "replies_to_hasan['renderedContent'] = replies_to_hasan['renderedContent'].apply(\n",
    "    tokenize).apply(lemmatize)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing\n",
    "\n",
    "Now we will do the most interesting part. We will use some of the NLP algorithms to calculate a sentiment and add this to our dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "replies_to_hasan['sentiment'] = replies_to_hasan['rawContent'].apply(\n",
    "    lambda text: sid.polarity_scores(' '.join(text))['compound'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the minimum and maximum sentiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9998 -0.9862\n"
     ]
    }
   ],
   "source": [
    "sentiment_max = replies_to_hasan['sentiment'].max()\n",
    "sentiment_min = replies_to_hasan['sentiment'].min()\n",
    "\n",
    "print(sentiment_max, sentiment_min)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print 5 most positive comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. ['wow', 'wow', 'wow', 'wow', 'wow', 'wow', 'wow', 'wow', 'wow', 'wow', 'wow', 'wow', 'wow', 'wow', 'wow', 'wow', 'wow', 'wow', 'wow', 'wow', 'wow', 'wow', 'wow', 'wow', 'wow', 'wow', 'wow', 'wow', 'wow', 'wow', 'wow', 'wow', 'wow', 'wow', 'wow', 'wow', 'wow', 'wow', 'wow', 'wow', 'wow', 'wow', 'wow', 'wow', 'wow', 'wow', 'wow', 'wow', 'wow', 'wow', 'wow', 'wow', 'wow', 'wow', 'wow', 'wow', 'wow', 'wow', 'wow', 'wow', 'wow', 'wow', 'wow', 'wow', 'wow', 'wow', 'wow', 'wow', 'wow', 'wow']\n",
      "2. ['believe', 'freedom', 'speech', 'believe', 'freedom', 'view', 'like', 'dictator', 'favor', 'freedom', 'speech', 'view', 'liked', 'favor', 'freedom', 'speech', 'favor', 'freedom', 'speech', 'precisely', 'view', 'despise', 'chomsky']\n",
      "3. ['beautiful', 'pup', 'soon', 'saw', 'obvious', 'two', 'meant', 'best', 'friend', 'yin', 'yang', 'congrats', 'hope', 'pup', 'brings', 'joy', 'laughter', 'love', 'year', 'come']\n",
      "4. ['fucking', 'love', 'shit', 'lmfao', 'love', 'tate', 'reason', 'loved', 'trump', 'office', 'entertainment', 'value', 'high', 'specially', 'jam', 'plus', 'watching', 'people', 'react', 'comment', 'lmfao', 'love']\n",
      "5. ['congratulation', 'puppy', 'gorgeous', 'look', 'like', 'found', 'exactly', 'pup', 'described', 'fate', 'brought', 'best', 'pet', 'ever', 'cat', 'noodle', 'lived', 'year', 'dna', 'test', 'great', 'idea', 'hope', 'many', 'many', 'happy', 'year', 'puppy', 'baby']\n"
     ]
    }
   ],
   "source": [
    "most_positive_replies = replies_to_hasan.sort_values(by='sentiment', ascending=False)[:5]\n",
    "\n",
    "for i in range(most_positive_replies.shape[0]):\n",
    "    print(f\"{i+1}. {most_positive_replies.iloc[i]['renderedContent']}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of them are gibberish two of them relate to Hasan getting a dog recently and the rest relate to political topics. Hasan's crowd at it's finest.\n",
    "\n",
    "Now for the less fun ones. Let's see what are the 5 most negative sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. ['ever', 'tried', 'purchase', 'firearm', 'doubt', 'know', 'nation', 'state', 'gun', 'law', 'doubt', 'rock', 'kill', 'abel', 'cain', 'mental', 'illness', 'problem', 'gun', 'problem', 'killing', 'child', 'gun', 'sick', 'person', 'messed']\n",
      "2. ['thing', 'leak', 'reveal', 'crime', 'injustice', 'u', 'government', 'reveals', 'critical', 'intel', 'regarding', 'war', 'put', 'ukrainian', 'civilian', 'danger', 'since', 'difficult', 'defend', 'country', 'like', 'war', 'crime', 'leak']\n",
      "3. ['saw', 'take', 'think', 'porn', 'better', 'advertiser', 'crypto', 'really', 'addiction', 'lead', 'problematic', 'behavior', 'probably', 'lead', 'lot', 'cultural', 'problem', 'better', 'global', 'currency', 'gtfoh', 'insane', 'hate', 'thing', 'insane', 'hate', 'gambling', 'stream', 'cool', 'porno', 'ad', 'hate', 'crypto', 'think', 'bad', 'believe', 'usd', 'bad', 'crypto', 'horrible', 'socialist', 'see', 'crypto', 'buffer', 'big', 'bank', 'insane', 'take']\n",
      "4. ['actually', 'half', 'gun', 'death', 'suicide', 'vast', 'majority', 'gun', 'murder', 'gang', 'violence', 'black', 'community', 'gun', 'problem']\n",
      "5. ['vast', 'majority', 'data', 'black', 'black', 'gun', 'violence', 'graph', 'child', 'death', 'black', 'shooter', 'would', 'completely', 'different', 'problem', 'gun', 'gun', 'violence', 'black', 'culture', 'stricter', 'gun', 'law', 'work', 'criminal', 'follow', 'law']\n"
     ]
    }
   ],
   "source": [
    "most_positive_replies = replies_to_hasan.sort_values(by='sentiment', ascending=True)[:5]\n",
    "\n",
    "for i in range(most_positive_replies.shape[0]):\n",
    "    print(f\"{i+1}. {most_positive_replies.iloc[i]['renderedContent']}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, three of them relate to guns in US. One is about the war in Ukraine and the last one relates to a problem of porn addiction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
